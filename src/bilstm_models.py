# -*- coding: utf-8 -*-
"""bilstm_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173zIwJN8n2g-Txw-wrfFZQSv7yoroxkJ

"""


import string

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from datasets import load_dataset
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import AutoTokenizer, BertTokenizerFast, TFAutoModel



# --------------- GLOBAL VARIABLES
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
chars = string.ascii_lowercase + string.ascii_uppercase + string.digits + string.punctuation
char2idx = {c: i+2 for i, c in enumerate(chars)}
char2idx["<PAD>"] = 0  # Special symbol for padding
char2idx["<UNK>"] = 1  # Special symbol for unknown character
max_word_len = 128  

# --------------- GLOBAL VARIABLES


def tokenize_and_align_labels(dataset):
  tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
  max_length = 128 # maximum length for padding/trimming
  label_all_tokens = True
  tokenized_inputs = tokenizer(dataset["tokens"], truncation=True,
                                is_split_into_words=True, padding='max_length',
                                max_length=max_length)

  labels = dataset["ner_tags"]
  aligned_labels = np.ones((max_length,), dtype=int) *10 #Padding token label

  # Replace labels by their corresponding value in `labels` or set to 10 (label for padding token)
  word_ids = tokenized_inputs.word_ids()
  previous_word_idx = None
  label_idx = 0

  for i, word_id in enumerate(word_ids):
      # Special tokens have a word_id that is None. We set the label to 10 so they are automatically
      # ignored in the loss function.
      if word_id is None:
          aligned_labels[i] = 10
      # We set the label for the first token of each word.        
      elif word_id != previous_word_idx:
          aligned_labels[i] = labels[label_idx]
          previous_word_idx = word_id
          label_idx += 1
      else:
          aligned_labels[i] = labels[label_idx - 1] if label_all_tokens else 10
  
  
  # use bert tokenizer to encode the characters
  char_encoded_tokens = tokenizer(dataset["tokens"],
                                  truncation=True,
                                  is_split_into_words=True,
                                  padding='max_length',
                                  max_length=max_length,
                                  return_offsets_mapping=True).offset_mapping



  return {"input_ids": tokenized_inputs["input_ids"], 
          "attention_mask": tokenized_inputs["attention_mask"],
          "labels": aligned_labels,
          "char_encoded_tokens": char_encoded_tokens
          }



def get_char_embed(word, max_word_len, char2idx):
  
  # Treat special tokens as exceptions
  if word in ["[CLS]", "[SEP]", "[PAD]", "[UNK]"]:
      return [0]*max_word_len

  char_embed = []
  for char in word:
      if char in char2idx:
          char_embed.append(char2idx[char])
      else:
          char_embed.append(char2idx['<UNK>'])
  
  # Pad the sequence
  if len(char_embed) < max_word_len:
      char_embed += [char2idx['<PAD>']] * (max_word_len - len(char_embed))
  else:
      char_embed = char_embed[:max_word_len]

  return char_embed



def get_sample_weights(labels, padding_label=10):
    return [0.0 if label == padding_label else 1.0 for label in labels]

# Prepare inputs for the model
def prepare_inputs(dataset):
    dataset = dataset.unbatch().as_numpy_iterator()

    input_ids = []
    attention_mask = []
    labels = []
    char_embeds = []
    
    sample_weights = []

    for example_tuple in dataset:
        example = example_tuple[0]  # Unpack the tuple
        label = example_tuple[1]

        input_ids.append(example['input_ids'])
        attention_mask.append(example['attention_mask'])
        labels.append(label)
        
        sample_weights.append(get_sample_weights(label))
        
        char_embed = []
        for word in example['input_ids']:
            # Decode the input_ids to get the original word
            word = tokenizer.decode([word])
            # Apply get_char_embed function to each word
            char_embed.append(get_char_embed(word, max_word_len, char2idx))
        char_embeds.append(char_embed)

    return (
        [np.array(input_ids), np.array(attention_mask), np.array(char_embeds)],
        np.array(labels),
        np.array(sample_weights)
            )


def evaluate_model(model, x_test, y_test):
  label2token = {
    'O': 0,
    'B-PER': 1,
    'I-PER': 2,
    'B-ORG': 3,
    'I-ORG': 4,
    'B-LOC': 5,
    'I-LOC': 6,
    'B-MISC': 7,
    'I-MISC': 8,
    '[PAD]': 10 
  
  }
  token2label = {v: k for k, v in label2token.items()}
  pred = model.predict(x_test)
  pred = np.argmax(pred, axis=-1)

  y_true = y_test
  y_pred = pred

  # Flattening and removing padding tokens
  y_true = y_true.flatten()
  y_pred = y_pred.flatten()

  mask = (y_true != 10) # assuming 10 is the padding token label
  y_true = y_true[mask]
  y_pred = y_pred[mask]
  y_true_str = [token2label[label] for label in y_true]
  y_pred_str = [token2label[label] for label in y_pred]

  print(classification_report(y_true_str, y_pred_str))




# Plot the training loss and accuracy
def plot_training(history,crf_model = False):
    loss = history.history['loss']
    if crf_model == True:
      val_loss = history.history['val_loss_val']
    else:
      val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.figure(figsize=(12,4))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, loss, 'b', label='Training loss')
    plt.plot(epochs, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot accuracy
    plt.subplot(1, 2, 2)
    acc = history.history['accuracy']
    if crf_model == True:
      val_acc = history.history['val_accuracy']
    else:
      val_acc = history.history['val_val_accuracy']
    plt.plot(epochs, acc, 'b', label='Training accuracy')
    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()


def evaluate_model(model, x_test, y_true, crf_model = False):
  label2token = {
    'O': 0,
    'B-PER': 1,
    'I-PER': 2,
    'B-ORG': 3,
    'I-ORG': 4,
    'B-LOC': 5,
    'I-LOC': 6,
    'B-MISC': 7,
    'I-MISC': 8,
    '[PAD]': 10 
  
  }
  token2label = {v: k for k, v in label2token.items()}
  y_pred = model.predict(x_test)
  
  if crf_model == False:
    y_pred = np.argmax(y_pred, axis=-1)

  # Flattening and removing padding tokens
  y_true = y_true.flatten()
  y_pred = y_pred.flatten()

  mask = (y_true != 10) # assuming 10 is the padding token label
  y_true = y_true[mask]
  y_pred = y_pred[mask]
  y_true_str = [token2label[label] for label in y_true]
  y_pred_str = [token2label[label] for label in y_pred]

  print(classification_report(y_true_str, y_pred_str))


