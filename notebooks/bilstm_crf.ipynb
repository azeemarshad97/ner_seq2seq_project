{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BiLSTM with character level information and extra CRF Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\notebooks\n",
      "Parent directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n",
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(f'Parent directory: {parent_dir}')\n",
    "os.chdir(parent_dir)\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "from transformers import BertTokenizerFast, TFAutoModel\n",
    "\n",
    "from src.bilstm_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/azeem/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023ba3a1609746a68924e2dec605ffaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\azeem\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-a055f85b27121498.arrow\n",
      "Loading cached processed dataset at C:\\Users\\azeem\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-debebc254be816c8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\azeem\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-be014cd55b1485b8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "[CLS] EU rejects German call to boycott British lamb. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Labels Before Transformation:\n",
      "[10  3  0  7  0  0  0  7  0  0  0 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10]\n",
      "\n",
      "Labels After Transformation:\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('conll2003')\n",
    "\n",
    "# Define label2token and token2label mappings\n",
    "label2token = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-ORG': 3,\n",
    "    'I-ORG': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    'B-MISC': 7,\n",
    "    'I-MISC': 8,\n",
    "    '[PAD]': 10 \n",
    "}\n",
    "token2label = {token: label for label, token in label2token.items()}\n",
    "\n",
    "# Process the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels)\n",
    "tokenized_datasets.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'labels', 'char_encoded_tokens'])\n",
    "\n",
    "# check the first example in the dataset\n",
    "example = tokenized_datasets['train'][0]\n",
    "\n",
    "# get the input_ids and convert to numpy array\n",
    "input_ids = example['input_ids'].numpy()\n",
    "\n",
    "# decode the input_ids to get the original sentence\n",
    "sentence = tokenizer.decode(input_ids)\n",
    "\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "\n",
    "# get the labels and convert to numpy array\n",
    "labels = example['labels'].numpy()\n",
    "\n",
    "# print the labels before transformation\n",
    "print(\"\\nLabels Before Transformation:\")\n",
    "print(labels)\n",
    "\n",
    "# remove the padding (10) before converting to labels\n",
    "labels = labels[labels != 10]\n",
    "\n",
    "# convert the tokenized labels back to their original string labels\n",
    "reconstructed_labels = [token2label[token] for token in labels]\n",
    "\n",
    "print(\"\\nLabels After Transformation:\")\n",
    "print(reconstructed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenized_data, shuffle=False, cache=True):\n",
    "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "  column_names = ['input_ids', 'attention_mask', 'labels', 'char_encoded_tokens']\n",
    "  print(f\"\\t preparing {tokenized_data} dataset ... \", flush=True)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((\n",
    "      {key: np.array(tokenized_datasets[tokenized_data][key]) for key in column_names},\n",
    "      np.array(tokenized_datasets[tokenized_data]['labels'])\n",
    "  ))\n",
    "\n",
    "  if cache:\n",
    "      dataset = dataset.cache()\n",
    "  if shuffle:\n",
    "      dataset = dataset.shuffle(1000)\n",
    "  dataset = dataset.batch(32, drop_remainder=True)\n",
    "  dataset = dataset.prefetch(AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = prepare_dataset('train', shuffle=True)\n",
    "validation_dataset = prepare_dataset('validation')\n",
    "test_dataset = prepare_dataset('test')\n",
    "\n",
    "print(f\"train dataset size: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"validation dataset size: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"test dataset size: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "print(f\"{tokenized_datasets['train'][0].keys()}\")\n",
    "\n",
    "print(f\"input_ids shape: {tokenized_datasets['train'][0]['input_ids'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['input_ids'])\n",
    "\n",
    "print(f\"attention maske shape: {tokenized_datasets['train'][0]['attention_mask'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['attention_mask'])\n",
    "\n",
    "print(f\"labels shape: {tokenized_datasets['train'][0]['labels'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['labels'])\n",
    "\n",
    "print(f\"tokenised train dataset shape: {tokenized_datasets['train'][0]['char_encoded_tokens'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['char_encoded_tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's prepare the model inputs\n",
    "input_data, labels, sample_weights = prepare_inputs(train_dataset)\n",
    "\n",
    "# Take one example from the input_data to showcase\n",
    "input_ids_example, attention_mask_example, char_embed_example = input_data[0][0], input_data[1][0], input_data[2][0]\n",
    "labels_example = labels[0]\n",
    "weights_example = sample_weights[0]\n",
    "\n",
    "# Decode the input_ids_example back to words  \n",
    "words = tokenizer.convert_ids_to_tokens(input_ids_example)\n",
    "\n",
    "# Display the first word, its input_ids, attention_mask, char_embed, label and sample_weight\n",
    "print(f\"Word: {words[0]}\")\n",
    "print(f\"Input IDs: {input_ids_example[0]}\")\n",
    "print(f\"Attention Mask: {attention_mask_example[0]}\")\n",
    "print(f\"Char Embedding: {char_embed_example[0]}\")\n",
    "print(f\"Label: {labels_example[0]}\")\n",
    "print(f\"Sample Weight: {weights_example[0]}\")\n",
    "\n",
    "x_train, y_train, sample_weights_train = prepare_inputs(train_dataset)\n",
    "x_test, y_test, sample_weights_test = prepare_inputs(test_dataset)\n",
    "x_val, y_val, sample_weights_val = prepare_inputs(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOAD_MODEL = False\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 75\n",
    "num_chars = len(char2idx) # The number of unique characters\n",
    "hidden_units = 96  # The number of hidden units for LSTM layers\n",
    "unique_labels = set([tag for sentence in dataset[\"train\"][\"ner_tags\"] for tag in sentence])\n",
    "num_classes = len(unique_labels)+1\n",
    "word_embedding_dim = 64  # The dimension of word embedding\n",
    "char_embed_dim = 20  # The dimension of character embedding\n",
    "vocab_size = len(tokenizer.get_vocab())  # Size of the vocabulary\n",
    "\n",
    "max_seq_len = 128  # Maximum number of words in a sequence\n",
    "max_word_len = 128  # Maximum number of characters in a word\n",
    "\n",
    "\n",
    "\n",
    "# prepare training data\n",
    "x_train_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_train]\n",
    "\n",
    "# y_train_encoded = [[label + 1 for label in seq] for seq in y_train]  # Shift labels to [1, 10]\n",
    "y_train_new = pad_sequences(y_train, maxlen=max_seq_len, padding=\"post\", value=10)  # Use 0 for padding\n",
    "\n",
    "x_val_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_val]\n",
    "# y_val_encoded = [[label + 1 for label in seq] for seq in y_val]\n",
    "y_val_new = pad_sequences(y_val, maxlen=max_seq_len, padding=\"post\", value=10)  # Use 0 for padding\n",
    "\n",
    "# only word\n",
    "x_train_nochar = [x_train_padded[0],x_train_padded[1]]\n",
    "x_val_nochar = [x_val_padded[0],x_val_padded[1]]\n",
    "\n",
    "\n",
    "print(f\"\\n\\n \\t\\t\\t creating and training model ... \\n\\n\")\n",
    "\n",
    "# ----------------- pre trained embedding\n",
    "# Load DistilBERT\n",
    "distilbert_model = TFAutoModel.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model.trainable = False\n",
    "\n",
    "# Character embedding\n",
    "print(f\"creating character embedding ... \")\n",
    "char_input = layers.Input(shape=(max_seq_len, max_word_len), dtype='int32')\n",
    "\n",
    "char_embed = layers.TimeDistributed(layers.Embedding(num_chars, char_embed_dim, mask_zero=True))(char_input)\n",
    "\n",
    "char_bilstm = layers.TimeDistributed(layers.Bidirectional(layers.LSTM(hidden_units)))(char_embed)\n",
    "\n",
    "# Word-level input\n",
    "print(f\"creating word-level input ... \")\n",
    "word_input = layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "attention_mask_input = layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "\n",
    "# Word Embedding\n",
    "print(f\"creating word embedding ... \")\n",
    "# word_embedding = layers.Embedding(input_dim=vocab_size, output_dim=word_embedding_dim)(word_input)\n",
    "word_embedding = distilbert_model([word_input, attention_mask_input])[0] # with distilbert\n",
    "\n",
    "# Concatenate word and char-level information\n",
    "print(f\"concatenating word and char-level information ... \")\n",
    "combined = layers.Concatenate()([word_embedding, char_bilstm])\n",
    "\n",
    "# Final BiLSTM layer for sequence tagging\n",
    "print(f\"final BiLSTM layer for sequence tagging ... \")\n",
    "bilstm = layers.Bidirectional(layers.LSTM(hidden_units, return_sequences=True))(combined)\n",
    "\n",
    "# for word embedding only\n",
    "# bilstm = layers.Bidirectional(layers.LSTM(hidden_units, return_sequences=True,\n",
    "#                                           kernel_initializer='he_normal'))(word_embedding)\n",
    "\n",
    "\n",
    "# Final BiLSTM layer for sequence tagging\n",
    "dropout = layers.Dropout(0.5)(bilstm)\n",
    "\n",
    "# Add CRF layer\n",
    "crf = CRF(num_classes+1)  # Plus 1 for the padding class\n",
    "output = crf(dropout)                                         \n",
    "\n",
    "\n",
    "base_model = tf.keras.Model(inputs=[word_input, attention_mask_input, char_input], outputs=output) # char info\n",
    "# base_model = tf.keras.Model(inputs=[word_input, attention_mask_input], outputs=output) #no char info\n",
    "model = ModelWithCRFLoss(base_model)\n",
    "\n",
    "\n",
    "class_weights = {i: 1 for i in range(num_classes)}\n",
    "class_weights[10] = 0  # Set the weight for the padding label to 0\n",
    "\n",
    "\n",
    "# Compile model\n",
    "print(f\"COMPILING MODEL ... \")\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "if LOAD_MODEL == True:\n",
    "  model.load_weights('./models/checkpoints/bilstm_crf_model_checkpoint.h5')\n",
    "else:\n",
    "  checkpoint = ModelCheckpoint('model_checkpoint.h5',\n",
    "                             verbose=1, save_best_only=True, mode='min',\n",
    "                             save_weights_only=True)\n",
    "\n",
    "\n",
    "# Training\n",
    "print(f\"TRAINING MODEL ... \")\n",
    "early_stopping = EarlyStopping(monitor='val_loss_val', patience=3, restore_best_weights=True)\n",
    "\n",
    "# get maximum workers from cpu\n",
    "import multiprocessing\n",
    "USE_MP = False\n",
    "workers = multiprocessing.cpu_count()\n",
    "\n",
    "if USE_MP == True:\n",
    "  history = model.fit(x_train_padded, y_train_new,\n",
    "                      validation_data = (x_val_padded,y_val_new),\n",
    "                      epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping],\n",
    "                      workers=workers-1)\n",
    "\n",
    "history = model.fit(x_train_padded, y_train_new,\n",
    "                    validation_data = (x_val_padded,y_val_new),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping])\n",
    "\n",
    "# history = model.fit(x_train_nochar, y_train,\n",
    "#                     validation_data = (x_val_nochar,y_val),\n",
    "#                     epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save weights\n",
    "model.save_weights('bilstm_crf_final_model.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_test]\n",
    "y_test_new = pad_sequences(y_test, maxlen=max_seq_len, padding=\"post\", value=10)  \n",
    "x_test_nochar = [x_test_padded[0],x_test_padded[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, x_test_padded, y_test_new, crf_model=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_training(history, crf_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
