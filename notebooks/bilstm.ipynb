{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BiLSTM with character level information **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\notebooks\n",
      "Parent directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n",
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(f'Parent directory: {parent_dir}')\n",
    "os.chdir(parent_dir)\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizerFast, TFAutoModel\n",
    "\n",
    "from src.bilstm_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/azeem/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b292e300dee14dd68e44cc5cfd759033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e9076ca6964cad9a9709154cf8dd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccf42acd9fe4b7f89d7c8e2aa4621fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffe0a25281b4683b550ddd9a17b3a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "[CLS] EU rejects German call to boycott British lamb. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Labels Before Transformation:\n",
      "[10  3  0  7  0  0  0  7  0  0  0 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10]\n",
      "\n",
      "Labels After Transformation:\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('conll2003')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# Define label2token and token2label mappings\n",
    "label2token = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-ORG': 3,\n",
    "    'I-ORG': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    'B-MISC': 7,\n",
    "    'I-MISC': 8,\n",
    "    '[PAD]': 10 \n",
    "}\n",
    "token2label = {token: label for label, token in label2token.items()}\n",
    "\n",
    "\n",
    "# Process the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels)\n",
    "tokenized_datasets.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'labels', 'char_encoded_tokens'])\n",
    "\n",
    "# check the first example in the dataset\n",
    "example = tokenized_datasets['train'][0]\n",
    "\n",
    "# get the input_ids and convert to numpy array\n",
    "input_ids = example['input_ids'].numpy()\n",
    "\n",
    "# decode the input_ids to get the original sentence\n",
    "sentence = tokenizer.decode(input_ids)\n",
    "\n",
    "print(\"Original Sentence:\")\n",
    "print(sentence)\n",
    "\n",
    "# get the labels and convert to numpy array\n",
    "labels = example['labels'].numpy()\n",
    "\n",
    "# print the labels before transformation\n",
    "print(\"\\nLabels Before Transformation:\")\n",
    "print(labels)\n",
    "\n",
    "# remove the padding (10) before converting to labels\n",
    "labels = labels[labels != 10]\n",
    "\n",
    "# convert the tokenized labels back to their original string labels\n",
    "reconstructed_labels = [token2label[token] for token in labels]\n",
    "\n",
    "print(\"\\nLabels After Transformation:\")\n",
    "print(reconstructed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokenized_data, shuffle=False, cache=True):\n",
    "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "  column_names = ['input_ids', 'attention_mask', 'labels', 'char_encoded_tokens']\n",
    "  print(f\"\\t preparing {tokenized_data} dataset ... \", flush=True)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((\n",
    "      {key: np.array(tokenized_datasets[tokenized_data][key]) for key in column_names},\n",
    "      np.array(tokenized_datasets[tokenized_data]['labels'])\n",
    "  ))\n",
    "\n",
    "  if cache:\n",
    "      dataset = dataset.cache()\n",
    "  if shuffle:\n",
    "      dataset = dataset.shuffle(1000)\n",
    "  dataset = dataset.batch(32, drop_remainder=True)\n",
    "  dataset = dataset.prefetch(AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t preparing train dataset ... \n",
      "\t preparing validation dataset ... \n",
      "\t preparing test dataset ... \n",
      "train dataset size: 14041\n",
      "validation dataset size: 3250\n",
      "test dataset size: 3453\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'char_encoded_tokens'])\n",
      "input_ids shape: (128,)\n",
      "attention maske shape: (128,)\n",
      "labels shape: (128,)\n",
      "tokenised train dataset shape: (128, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = prepare_dataset('train', shuffle=True)\n",
    "validation_dataset = prepare_dataset('validation')\n",
    "test_dataset = prepare_dataset('test')\n",
    "\n",
    "print(f\"train dataset size: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"validation dataset size: {len(tokenized_datasets['validation'])}\")\n",
    "print(f\"test dataset size: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "print(f\"{tokenized_datasets['train'][0].keys()}\")\n",
    "\n",
    "print(f\"input_ids shape: {tokenized_datasets['train'][0]['input_ids'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['input_ids'])\n",
    "\n",
    "print(f\"attention maske shape: {tokenized_datasets['train'][0]['attention_mask'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['attention_mask'])\n",
    "\n",
    "print(f\"labels shape: {tokenized_datasets['train'][0]['labels'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['labels'])\n",
    "\n",
    "print(f\"tokenised train dataset shape: {tokenized_datasets['train'][0]['char_encoded_tokens'].shape}\")\n",
    "# print(tokenized_datasets['train'][0]['char_encoded_tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [CLS]\n",
      "Input IDs: 101\n",
      "Attention Mask: 1\n",
      "Char Embedding: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Label: 10\n",
      "Sample Weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "# First, let's prepare the model inputs\n",
    "input_data, labels, sample_weights = prepare_inputs(train_dataset)\n",
    "\n",
    "# Take one example from the input_data to showcase\n",
    "input_ids_example, attention_mask_example, char_embed_example = input_data[0][0], input_data[1][0], input_data[2][0]\n",
    "labels_example = labels[0]\n",
    "weights_example = sample_weights[0]\n",
    "\n",
    "# Decode the input_ids_example back to words  \n",
    "words = tokenizer.convert_ids_to_tokens(input_ids_example)\n",
    "\n",
    "# Display the first word, its input_ids, attention_mask, char_embed, label and sample_weight\n",
    "print(f\"Word: {words[0]}\")\n",
    "print(f\"Input IDs: {input_ids_example[0]}\")\n",
    "print(f\"Attention Mask: {attention_mask_example[0]}\")\n",
    "print(f\"Char Embedding: {char_embed_example[0]}\")\n",
    "print(f\"Label: {labels_example[0]}\")\n",
    "print(f\"Sample Weight: {weights_example[0]}\")\n",
    "\n",
    "x_train, y_train, sample_weights_train = prepare_inputs(train_dataset)\n",
    "x_test, y_test, sample_weights_test = prepare_inputs(test_dataset)\n",
    "x_val, y_val, sample_weights_val = prepare_inputs(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 75\n",
    "num_chars = len(char2idx) # The number of unique characters\n",
    "hidden_units = 96  # The number of hidden units for LSTM layers\n",
    "unique_labels = set([tag for sentence in dataset[\"train\"][\"ner_tags\"] for tag in sentence])\n",
    "num_classes = len(unique_labels)+1\n",
    "word_embedding_dim = 64  # The dimension of word embedding\n",
    "char_embed_dim = 20  # The dimension of character embedding\n",
    "vocab_size = len(tokenizer.get_vocab())  # Size of the vocabulary\n",
    "\n",
    "max_seq_len = 128  # Maximum number of words in a sequence\n",
    "max_word_len = 128  # Maximum number of characters in a word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_train_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_train]\n",
    "\n",
    "# y_train_encoded = [[label + 1 for label in seq] for seq in y_train]  # Shift labels to [1, 10]\n",
    "y_train_new = pad_sequences(y_train, maxlen=max_seq_len, padding=\"post\", value=10)  # Use 0 for padding\n",
    "\n",
    "x_val_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_val]\n",
    "# y_val_encoded = [[label + 1 for label in seq] for seq in y_val]\n",
    "y_val_new = pad_sequences(y_val, maxlen=max_seq_len, padding=\"post\", value=10)  # Use 0 for padding\n",
    "\n",
    "# only word\n",
    "x_train_nochar = [x_train_padded[0],x_train_padded[1]]\n",
    "x_val_nochar = [x_val_padded[0],x_val_padded[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \t\t\t creating and training model ... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e9e11c1fa14da3b8f63195ece3a505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\azeem\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating character embedding ... \n",
      "creating word-level input ... \n",
      "creating word embedding ... \n",
      "concatenating word and char-level information ... \n",
      "final BiLSTM layer for sequence tagging ... \n",
      "COMPILING MODEL ... \n",
      "TRAINING MODEL ... \n",
      "Epoch 1/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.1121 \n",
      "Epoch 1: val_loss improved from inf to 0.96185, saving model to model_checkpoint.h5\n",
      "187/187 [==============================] - 3595s 19s/step - loss: 1.1121 - val_loss: 0.9618\n",
      "Epoch 2/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.8230 \n",
      "Epoch 2: val_loss improved from 0.96185 to 0.71966, saving model to model_checkpoint.h5\n",
      "187/187 [==============================] - 3672s 20s/step - loss: 0.8230 - val_loss: 0.7197\n",
      "Epoch 3/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.6737 \n",
      "Epoch 3: val_loss improved from 0.71966 to 0.62143, saving model to model_checkpoint.h5\n",
      "187/187 [==============================] - 3700s 20s/step - loss: 0.6737 - val_loss: 0.6214\n",
      "Epoch 4/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5963 \n",
      "Epoch 4: val_loss improved from 0.62143 to 0.55720, saving model to model_checkpoint.h5\n",
      "187/187 [==============================] - 3711s 20s/step - loss: 0.5963 - val_loss: 0.5572\n",
      "Epoch 5/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.5422 \n",
      "Epoch 5: val_loss improved from 0.55720 to 0.51212, saving model to model_checkpoint.h5\n",
      "187/187 [==============================] - 3716s 20s/step - loss: 0.5422 - val_loss: 0.5121\n",
      "Epoch 6/20\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.4994 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTRAINING MODEL ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 70\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train_padded, y_train_new,\n\u001b[0;32m     71\u001b[0m                     validation_data \u001b[39m=\u001b[39;49m (x_val_padded,y_val_new),\n\u001b[0;32m     72\u001b[0m                     epochs\u001b[39m=\u001b[39;49mEPOCHS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, callbacks \u001b[39m=\u001b[39;49m [checkpoint, early_stopping])\n\u001b[0;32m     73\u001b[0m \u001b[39m# history = model.fit(x_train_nochar, y_train,\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m#                     validation_data = (x_val_nochar,y_val),\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m#                     epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping])\u001b[39;00m\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\keras\\engine\\training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1716\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1717\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1728\u001b[0m     )\n\u001b[1;32m-> 1729\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1730\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1731\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1732\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1733\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1734\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1735\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1736\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1737\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1738\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1739\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1740\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1741\u001b[0m )\n\u001b[0;32m   1742\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1743\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1744\u001b[0m }\n\u001b[0;32m   1745\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\keras\\engine\\training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   2069\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m   2070\u001b[0m ):\n\u001b[0;32m   2071\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2072\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2074\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n\\n \\t\\t\\t creating and training model ... \\n\\n\")\n",
    "\n",
    "# ----------------- pre trained embedding\n",
    "# Load DistilBERT\n",
    "distilbert_model = TFAutoModel.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model.trainable = False\n",
    "\n",
    "# Character embedding\n",
    "print(f\"creating character embedding ... \")\n",
    "char_input = layers.Input(shape=(max_seq_len, max_word_len), dtype='int32')\n",
    "\n",
    "char_embed = layers.TimeDistributed(layers.Embedding(num_chars, char_embed_dim, mask_zero=True))(char_input)\n",
    "\n",
    "char_bilstm = layers.TimeDistributed(layers.Bidirectional(layers.LSTM(hidden_units)))(char_embed)\n",
    "\n",
    "# Word-level input\n",
    "print(f\"creating word-level input ... \")\n",
    "word_input = layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "attention_mask_input = layers.Input(shape=(max_seq_len,), dtype='int32')\n",
    "\n",
    "# Word Embedding\n",
    "print(f\"creating word embedding ... \")\n",
    "# word_embedding = layers.Embedding(input_dim=vocab_size, output_dim=word_embedding_dim)(word_input)\n",
    "word_embedding = distilbert_model([word_input, attention_mask_input])[0] # with distilbert\n",
    "\n",
    "# Concatenate word and char-level information\n",
    "print(f\"concatenating word and char-level information ... \")\n",
    "combined = layers.Concatenate()([word_embedding, char_bilstm])\n",
    "\n",
    "# Final BiLSTM layer for sequence tagging\n",
    "print(f\"final BiLSTM layer for sequence tagging ... \")\n",
    "bilstm = layers.Bidirectional(layers.LSTM(hidden_units, return_sequences=True))(combined)\n",
    "\n",
    "# for word embedding only\n",
    "# bilstm = layers.Bidirectional(layers.LSTM(hidden_units, return_sequences=True,\n",
    "#                                           kernel_initializer='he_normal'))(word_embedding)\n",
    "\n",
    "# Final BiLSTM layer for sequence tagging\n",
    "dropout = layers.Dropout(0.5)(bilstm)\n",
    "output = layers.TimeDistributed(layers.Dense(num_classes+1, activation='softmax',\n",
    "                                             kernel_initializer='he_normal'))(dropout)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[word_input, attention_mask_input, char_input], outputs=output) # char info\n",
    "# model = tf.keras.Model(inputs=[word_input, attention_mask_input], outputs=output) #no char info\n",
    "\n",
    "class_weights = {i: 1 for i in range(num_classes)}\n",
    "class_weights[10] = 0  # Set the weight for the padding label to 0\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Compile model\n",
    "print(f\"COMPILING MODEL ... \")\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss = loss_fn)\n",
    "\n",
    "if LOAD_MODEL == True:\n",
    "  model.load_weights('./models/checkpoints/bilstm_checkpoint_model.h5')\n",
    "else:\n",
    "  checkpoint = ModelCheckpoint('model_checkpoint.h5', monitor='val_loss',\n",
    "                             verbose=1, save_best_only=True, mode='min',\n",
    "                             save_weights_only=True)\n",
    "\n",
    "\n",
    "# Training\n",
    "print(f\"TRAINING MODEL ... \")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# get maximum workers from cpu\n",
    "import multiprocessing\n",
    "USE_MP = False\n",
    "workers = multiprocessing.cpu_count()\n",
    "\n",
    "if USE_MP == True:\n",
    "  history = model.fit(x_train_padded, y_train_new,\n",
    "                      validation_data = (x_val_padded,y_val_new),\n",
    "                      epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping], \n",
    "                      use_multiprocessing=True, workers=workers-1)\n",
    "\n",
    "history = model.fit(x_train_padded, y_train_new,\n",
    "                    validation_data = (x_val_padded,y_val_new),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping])\n",
    "\n",
    "  \n",
    "  \n",
    "# history = model.fit(x_train_nochar, y_train,\n",
    "#                     validation_data = (x_val_nochar,y_val),\n",
    "#                     epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks = [checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('./models/bilstm_final_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded = [pad_sequences(x, maxlen=max_seq_len, padding=\"post\", value=vocab_size) for x in x_test]\n",
    "y_test_new = pad_sequences(y_test, maxlen=max_seq_len, padding=\"post\", value=10)  \n",
    "x_test_nochar = [x_test_padded[0],x_test_padded[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 574s 5s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.65      0.46      0.54      2985\n",
      "      B-MISC       0.38      0.09      0.15      1246\n",
      "       B-ORG       0.45      0.53      0.49      3479\n",
      "       B-PER       0.53      0.48      0.50      2974\n",
      "       I-LOC       0.93      0.03      0.07       413\n",
      "      I-MISC       0.08      0.03      0.05       319\n",
      "       I-ORG       0.42      0.24      0.30      1295\n",
      "       I-PER       0.63      0.74      0.68      2702\n",
      "           O       0.92      0.97      0.94     47529\n",
      "\n",
      "    accuracy                           0.84     62942\n",
      "   macro avg       0.55      0.40      0.41     62942\n",
      "weighted avg       0.83      0.84      0.83     62942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, x_test_padded, y_test_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_training(history,crf_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
