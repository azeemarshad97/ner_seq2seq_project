{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\\notebooks\n",
      "Parent directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n",
      "Current working directory: d:\\azeem\\Documents\\UNIGE\\MSc CS\\Semester IV\\METL\\ner_seq2seq_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.neural_baseline import *\n",
    "from src.utils.conlleval import *\n",
    "\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(f'Parent directory: {parent_dir}')\n",
    "os.chdir(parent_dir)\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "%autoreload 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset, ner_model, mapping):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "    \n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x, verbose=0)  # set verbose to 0\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "    \n",
    "    res = evaluate(real_tags, predicted_tags, verbose = True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline n°1: Neural Network Implementation with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data and preparing vocabulary of size 20000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/azeem/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a785b2f6cd7437fbd35b0adeae2fcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets...\n",
      "creating model...\n",
      "\n",
      "training model...\n",
      "\n",
      "Epoch 1/10\n",
      "439/439 [==============================] - 10s 18ms/step - loss: 0.6439\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.2483\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.1514\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.1172\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.0952\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.0776\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.0658\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 7s 17ms/step - loss: 0.0564\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 7s 17ms/step - loss: 0.0491\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 7s 17ms/step - loss: 0.0432\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "sample_text = \"eu rejects german call to boycott british lamb\"\n",
    "\n",
    "\n",
    "print(f\"processing data and preparing vocabulary of size {vocab_size}...\")    \n",
    "conll_data = load_and_prepare_data()\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "\n",
    "# vocab_size = 20000\n",
    "vocabulary = get_vocabulary(conll_data, vocab_size)\n",
    "\n",
    "print(f\"preparing datasets...\")\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "\n",
    "# batch_size = 32\n",
    "train_dataset, val_dataset = prepare_datasets(vocabulary, batch_size)\n",
    "\n",
    "num_tags = len(mapping)\n",
    "\n",
    "print(f\"creating model...\\n\")\n",
    "ner_model = create_model(num_tags, vocab_size)\n",
    "\n",
    "print(f\"training model...\\n\")\n",
    "compile_and_fit(ner_model, train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 212ms/step\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n",
      "\n",
      "calculating metrics...\n",
      "\n",
      "processed 51362 tokens with 5942 phrases; found: 5147 phrases; correct: 3845.\n",
      "accuracy:  61.61%; (non-O)\n",
      "accuracy:  93.29%; precision:  74.70%; recall:  64.71%; FB1:  69.35\n",
      "              LOC: precision:  82.26%; recall:  81.00%; FB1:  81.62  1809\n",
      "             MISC: precision:  74.94%; recall:  67.14%; FB1:  70.82  826\n",
      "              ORG: precision:  69.29%; recall:  57.20%; FB1:  62.66  1107\n",
      "              PER: precision:  69.11%; recall:  52.71%; FB1:  59.81  1405\n",
      "\n",
      "\n",
      "precision: \t74.70\n",
      "   recall: \t64.71\n",
      "       f1: \t69.35\n"
     ]
    }
   ],
   "source": [
    "print(predict_sample(ner_model, sample_text, mapping, lookup_layer))\n",
    "\n",
    "print(f\"\\ncalculating metrics...\\n\")\n",
    "res = calculate_metrics(val_dataset, ner_model, mapping)\n",
    "\n",
    "# res is a tuple of (precision, recall, f1), print it out beautifully\n",
    "print(\"\\n\")\n",
    "print(f\"precision: \\t{res[0]:.2f}\")\n",
    "print(f\"   recall: \\t{res[1]:.2f}\")\n",
    "print(f\"       f1: \\t{res[2]:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline n°2: CRF Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.crf_baseline import * \n",
    "from IPython.display import display\n",
    "\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'NNP', 'B-NP', 'B-ORG'],\n",
       " ['rejects', 'VBZ', 'B-VP', 'O'],\n",
       " ['German', 'JJ', 'B-NP', 'B-MISC'],\n",
       " ['call', 'NN', 'I-NP', 'O'],\n",
       " ['to', 'TO', 'B-VP', 'O'],\n",
       " ['boycott', 'VB', 'I-VP', 'O'],\n",
       " ['British', 'JJ', 'B-NP', 'B-MISC'],\n",
       " ['lamb', 'NN', 'I-NP', 'O'],\n",
       " ['.', '.', 'O', 'O']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_raw_input(filename):\n",
    "    \"\"\"Read a train/test file and return the contents as a list of list of lists. \n",
    "    \n",
    "    The innermost list is a record of 4 items, one per word.\n",
    "    The middle-level list contains all the records in one sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    all_items = []\n",
    "\n",
    "    with open(filename) as fh:\n",
    "        current_item = []\n",
    "        all_items.append(current_item)\n",
    "\n",
    "        for line in fh:\n",
    "            tags = line.strip().split()\n",
    "            if len(tags) == 0 or tags[0] == '-DOCSTART-':\n",
    "                continue\n",
    "            current_item.append(tags)\n",
    "            if tags[0] == '.' and tags[1] == '.':\n",
    "                current_item = []\n",
    "                all_items.append(current_item)\n",
    "                \n",
    "    return all_items\n",
    "\n",
    "train_sents = read_raw_input('./data/CoNLL-2003_train.txt')\n",
    "test_sents = read_raw_input('./data/CoNLL-2003_test.txt')\n",
    "\n",
    "display(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>parse</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_seq_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rejects</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>German</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>call</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>B-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boycott</td>\n",
       "      <td>VB</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lamb</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  pos parse     ner\n",
       "word_seq_num                            \n",
       "0                  EU  NNP  B-NP   B-ORG\n",
       "1             rejects  VBZ  B-VP       O\n",
       "2              German   JJ  B-NP  B-MISC\n",
       "3                call   NN  I-NP       O\n",
       "4                  to   TO  B-VP       O\n",
       "5             boycott   VB  I-VP       O\n",
       "6             British   JJ  B-NP  B-MISC\n",
       "7                lamb   NN  I-NP       O\n",
       "8                   .    .     O       O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = all_sentences(train_sents)\n",
    "test_sents  = all_sentences(test_sents)\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7375/7375 [02:50<00:00, 43.17it/s]\n",
      "100%|██████████| 1627/1627 [00:38<00:00, 41.98it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = get_feature_values(train_sents)\n",
    "X_test = get_feature_values(test_sents)\n",
    "y_train, y_test = get_labels(train_sents), get_labels(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 37.3 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=200,\n",
    "    verbose=False,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " flat f1 score: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.870     0.839     0.854      1668\n",
      "       I-LOC      0.801     0.720     0.758       257\n",
      "      B-MISC      0.800     0.748     0.773       702\n",
      "      I-MISC      0.628     0.657     0.643       216\n",
      "       B-ORG      0.802     0.723     0.761      1661\n",
      "       I-ORG      0.655     0.734     0.692       835\n",
      "       B-PER      0.829     0.853     0.841      1617\n",
      "       I-PER      0.867     0.947     0.905      1156\n",
      "\n",
      "   micro avg      0.809     0.806     0.808      8112\n",
      "   macro avg      0.782     0.778     0.778      8112\n",
      "weighted avg      0.811     0.806     0.807      8112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "f1_score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
    "print(f\" flat f1 score: {f1_score:.2f}\")\n",
    "\n",
    "report = calculate_metrics_crf(y_test, y_pred, labels)\n",
    "print(f\"{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
